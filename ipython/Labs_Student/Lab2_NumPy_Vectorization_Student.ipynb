{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll generate a random matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Number of columns (features)\n",
    "K = 5\n",
    "\n",
    "#Number of records\n",
    "N = 1000\n",
    "\n",
    "#Generate an NxK matrix of uniform random variables\n",
    "X = np.random.random([N, K])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's peak at our data to confirm it looks as we expect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.88269874,  0.79991471,  0.21936384,  0.21760988,  0.24162985])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[100, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is about designing a scoring function for a logistic regression. As we are not concerned with fitting a model to data, we can just make up a logistic regression. <br> <br>\n",
    "\n",
    "For quick intro, the Logistic Regression takes the form of $\\hat{Y} = f(x * \\beta^T)$, where $x$ is the $1xK$ vector of features and $\\beta$ is the $1xK$ vector of weights. The function $f$, called a 'link' function, is the inverse logit: <br><br>\n",
    "\n",
    "<center>$f(a)=\\frac{1}{1+e^{-a}}$</center> <br><br>\n",
    "\n",
    "In this notebook we'll write a function that, given inputs of $X$ and $\\beta$, returns a value for $\\hat{Y}$.\n",
    "<br><br>\n",
    "First let's generate a random set of weights to represent $\\beta$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.57366105,  0.30701395, -0.90111119, -0.21073918, -0.81685711])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = 2 * (np.random.random(K) - 0.5)\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we applied a neat NumPy trick here. The numpy.random.random() function returns an array, yet we applied what appears to be a scalar operation on the vector. This is an example of what NumPy calls vectorization (a major point of this tutorial), which offers us both a very fast way to do run vector computations as well as a clean and concise method of coding. \n",
    "\n",
    "<br><br>\n",
    "\n",
    "<b>Question: we designed the above $beta$ vector such that $E[\\beta_i]=0$. How can we confirm that we did this correctly?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.43907091639474505"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start by taking the mean of the beta we already calculated\n",
    "beta.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#It is likely the above is not equal to zero. Let's simulate this 100k times and see what the average mean is\n",
    "\n",
    "means = []\n",
    "for i in range(100000):\n",
    "    means.append(2 * (np.random.random(K) - 0.5).mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use matplotlibs hist function to plot the histogram of means here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEACAYAAAC3adEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nv98U+i+9wwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1097622d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(means)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should expect the distribution to be centered around zero. Is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write our scoring function. Let's try to use as much of Numpy's inner optimization as possible (hint, this can be done in two lines and without writing any loops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_logistic_regression(X, beta):\n",
    "    '''\n",
    "    This function takes in an NxK matrix X and 1xK vector beta.\n",
    "    The function should apply the logistic scoring function to each record of X.\n",
    "    The output should be an Nx1 vector of scores\n",
    "    '''\n",
    "    \n",
    "    #First let's calculate X*beta - make sure to use numpy's 'dot' method\n",
    "    xbeta = X.dot(beta)\n",
    "    \n",
    "    #Now let's input this into the link function\n",
    "    prob_score = 1 / (1 + np.exp(-1 * xbeta))\n",
    "    \n",
    "    return prob_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how much faster is it by using Numpy? We can test this be writing the same function that uses no Numpy and executes via loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_logistic_regression_NoNumpy(X, beta):\n",
    "    '''\n",
    "    This function takes in an NxK matrix X and 1xK vector beta.\n",
    "    The function should apply the logistic scoring function to each record of X.\n",
    "    The output should be an Nx1 vector of scores\n",
    "    '''\n",
    "    #Let's calculate xbeta using loops\n",
    "    xbeta = []\n",
    "    for row in X:\n",
    "        \n",
    "        xb = 0\n",
    "        for i, el in enumerate(row):\n",
    "            xb += el * beta[i]\n",
    "        \n",
    "        xbeta.append(xb)\n",
    "        \n",
    "    #Now let's apply the link function to each xbeta\n",
    "    prob_score = []\n",
    "    for xb in xbeta:\n",
    "        prob_score.append(1 / (1 + np.exp(-1 * xb)))\n",
    "        \n",
    "    return prob_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing any analysis, let's test the output of each to make sure they equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = np.abs(score_logistic_regression_NoNumpy(X, beta) - score_logistic_regression(X, beta))\n",
    "np.round(diff.sum(), 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If they equal then we can proceed with timing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 4.14 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit score_logistic_regression_NoNumpy(X, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 10.32 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "10000 loops, best of 3: 20.5 Âµs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit score_logistic_regression(X, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about vectorization, check out the following links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
