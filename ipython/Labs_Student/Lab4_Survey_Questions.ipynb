{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/briand/Desktop/ds course/ipython/data/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#We assume data is in a parallel directory to this one called 'data'\n",
    "cwd = os.getcwd()\n",
    "datadir = '/'.join(cwd.split('/')[0:-1]) + '/data/'\n",
    "print(datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Student put in read data command here:\n",
    "data = pd.read_csv(datadir + 'survey_responses_2018.csv', header = 0, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the column headers and use something more descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'cs_python', 'cs_java', 'cs_c', 'cs_perl', 'cs_javascript',\n",
       "       'cs_r', 'cs_sas', 'profile_1', 'profile_2', 'profile_3', 'profile_4',\n",
       "       'profile_5', 'profile_6', 'profile_7', 'fruit', 'len_answer', 'season',\n",
       "       'experience_coded', 'experience'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Student put in code to look at column names\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column names like 'profile_1-profile_7' aren't very descriptive. As a quick data maintenance task, let's rename the columns starting with 'profile'. The dictionary in the next cell maps the integer index to a descriptive text.\n",
    "\n",
    "Tactically, let's loop through each column name. Within the loop let's check whether the column name starts with 'profile.' If it does, let's create a new name that swaps the key with the value using profile_mapping dictionary (i.e., profile_1 -> profile_Viz). We then add the new column name to a list. If it doesn't start with 'profile' just add the old column name to the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "profile_mapping = {1:'Viz',\n",
    "                   2:'CS',\n",
    "                   3:'Math',\n",
    "                   4:'Stats',\n",
    "                   5:'ML',\n",
    "                   6:'Bus',\n",
    "                   7:'Com'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Student put code here to change the header names\n",
    "newcols = []\n",
    "\n",
    "for colname in data.columns:\n",
    "    \n",
    "    if colname[0:7] == 'profile':\n",
    "        \n",
    "        newcols.append('profile_{}'.format(profile_mapping[int(colname[-1])]))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        newcols.append(colname)\n",
    "    \n",
    "#Now swap the old columns with the values in newcols    \n",
    "data.columns = newcols    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this data to illustrate common data analytic techniques. We have one numeric variable (len_answer) and different categorical variables which may carry some signal of the 'len_answer' variable. \n",
    "\n",
    "'Len_answer' is the character count of the response to the following question: \"Besides the examples given in lecture 1, discuss a case where data science has created value for some company. Please explain the company's goals and how any sort of data analysis could have helped the company achieve said goals.\" As this is a subjective business question, let's hypothesize that students with more professional experience might be more likely to give longer answers. \n",
    "\n",
    "In more technical terms, we'll test whether the variance of len_answer can be explained away by the categorical representation of a student's experience. \n",
    "\n",
    "The first thing we should do is look at the distribution of len_answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([30., 68., 34., 14.,  7.,  2.,  1.,  1.,  0.,  1.]),\n",
       " array([   0. ,  298.9,  597.8,  896.7, 1195.6, 1494.5, 1793.4, 2092.3,\n",
       "        2391.2, 2690.1, 2989. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Student - build and plot a histogram here\n",
    "plt.hist(data.len_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we may have at least one strong outlier and somewhat of a log-normal distribution. Let's also use the Pandas describe() method to get a stronger sense of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     158.000000\n",
       "mean      589.708861\n",
       "std       417.568947\n",
       "min         0.000000\n",
       "25%       327.500000\n",
       "50%       497.500000\n",
       "75%       726.500000\n",
       "max      2989.000000\n",
       "Name: len_answer, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.len_answer.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider cleaning up the data. We'll remove the top k values as well as those with a length less than 50 (which we think is a generous minimum to communicate a reasonable answer.\n",
    "\n",
    "Create a new data_frame that removes these outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((152, 20), (158, 20))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write a function to get the kth largest value of an array\n",
    "def get_kth_largest(inarray, k):\n",
    "    inarray.sort()\n",
    "    return inarray[-k]\n",
    "\n",
    "k = 3\n",
    "kth_largest = get_kth_largest(np.array(data.len_answer.values), 3)\n",
    "#Question = why did we wrap the series into an np.array() call in the above function call?\n",
    "\n",
    "#Student create a filtered data frame here\n",
    "outlier_filter = (data.len_answer > 50) & (data.len_answer < kth_largest)\n",
    "data_clean = data[outlier_filter]\n",
    "\n",
    "#Compare the shape of both dataframes\n",
    "data_clean.shape, data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have cleaned our data, let's run a pairwise t-test on each experience level to see if their difference in len_answer is statistically significant. To run a t-test, we'll need the mean, standard-deviation and count for each group. We can achieve this with a pandas groupby operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">len_answer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experience</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-5 years, I'm getting good at what I do!</th>\n",
       "      <td>644.852941</td>\n",
       "      <td>316.607599</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5+ years, I'm a veteran!</th>\n",
       "      <td>607.785714</td>\n",
       "      <td>427.640248</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt; 2 years, I'm fresh!</th>\n",
       "      <td>460.941176</td>\n",
       "      <td>195.111303</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>None, I just finished my undergrad!</th>\n",
       "      <td>567.128571</td>\n",
       "      <td>344.456258</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           len_answer                  \n",
       "                                                 mean         std count\n",
       "experience                                                             \n",
       "2-5 years, I'm getting good at what I do!  644.852941  316.607599    34\n",
       "5+ years, I'm a veteran!                   607.785714  427.640248    14\n",
       "< 2 years, I'm fresh!                      460.941176  195.111303    34\n",
       "None, I just finished my undergrad!        567.128571  344.456258    70"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Student input code here\n",
    "data_clean_grouped = data_clean[['len_answer', 'experience']].groupby(['experience']).agg(['mean', 'std', 'count'])\n",
    "data_clean_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, we can see a potential split between the [0, 2] year experience range and the [2+] experience range. Let's be more rigorous and run t-tests. Let's write a function that takes in the necessary statistics and returns a p-value.\n",
    "\n",
    "Remember, the t-stat for the difference between two means is:\n",
    "\n",
    "<center>$t = \\frac{\\hat{\\mu_1} - \\hat{\\mu_2}}{\\sqrt{\\frac{\\hat{\\sigma_1}^2}{n_1} + \\frac{\\hat{\\sigma_2}^2}{n_2}}}$</center>\n",
    "\n",
    "The p-value can be found using a t-distribution, but for simplicity, let's approximate this with the normal distribution. For the 2-tailed test, the p-value is: 2 * (1 - Norm.CDF(T))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Student complete the function\n",
    "from scipy.stats import norm\n",
    "\n",
    "def pvalue_diffmeans_twotail(mu1, sig1, n1, mu2, sig2, n2):\n",
    "    '''\n",
    "    P-value calculator for the hypothesis test of mu1 != mu2.\n",
    "    Takes in the approprate inputs to compute the t-statistic for the difference between means\n",
    "    Outputs a p-value for a two-sample t-test.\n",
    "    '''\n",
    "    diff = mu1 - mu2\n",
    "    stderror = np.sqrt(sig1**2 / n1 + sig2**2 / n2)\n",
    "    t = diff / stderror\n",
    "    \n",
    "    p_value = 2 * (1- norm.cdf(np.abs(t)))\n",
    "    \n",
    "    return (t, p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now loop through all possible pairs in data_clean_grouped and perform a t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two tailed T-Test between groups: 2-5 years, I'm getting good at what I do! and 5+ years, I'm a veteran!\n",
      "Diff = 37.0 characters\n",
      "The t-stat is 0.293 and p-value is 0.77\n",
      "\n",
      "Two tailed T-Test between groups: 2-5 years, I'm getting good at what I do! and < 2 years, I'm fresh!\n",
      "Diff = 184.0 characters\n",
      "The t-stat is 2.884 and p-value is 0.004\n",
      "\n",
      "Two tailed T-Test between groups: 2-5 years, I'm getting good at what I do! and None, I just finished my undergrad!\n",
      "Diff = 78.0 characters\n",
      "The t-stat is 1.141 and p-value is 0.254\n",
      "\n",
      "Two tailed T-Test between groups: 5+ years, I'm a veteran! and < 2 years, I'm fresh!\n",
      "Diff = 147.0 characters\n",
      "The t-stat is 1.233 and p-value is 0.218\n",
      "\n",
      "Two tailed T-Test between groups: 5+ years, I'm a veteran! and None, I just finished my undergrad!\n",
      "Diff = 41.0 characters\n",
      "The t-stat is 0.335 and p-value is 0.738\n",
      "\n",
      "Two tailed T-Test between groups: < 2 years, I'm fresh! and None, I just finished my undergrad!\n",
      "Diff = -106.0 characters\n",
      "The t-stat is -2.002 and p-value is 0.045\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Student put in code here:\n",
    "\n",
    "#get distinct values in the data frame for the experience variable\n",
    "\n",
    "#data_grouped = data[['len_answer', 'experience']].groupby(['experience']).agg(['mean', 'std', 'count'])\n",
    "#ttest_data = data_grouped\n",
    "\n",
    "\n",
    "ttest_data = data_clean_grouped\n",
    "\n",
    "\n",
    "grps = ttest_data.index.values\n",
    "\n",
    "#Now loop through each pair\n",
    "for i, grp1 in enumerate(grps):\n",
    "    for grp2 in grps[i + 1:]:\n",
    "    \n",
    "        '''\n",
    "        hint: since the grp name is the index, pull out the record corresponding to that index value. \n",
    "        Also, the result of groupby uses a multi-index. So be sure to index on 'len_answer' as well.\n",
    "        Then pull out the mean, std, and cnt from that result.   \n",
    "        '''        \n",
    "        row1 = ttest_data.loc[grp1].loc['len_answer']\n",
    "        row2 = ttest_data.loc[grp2].loc['len_answer']\n",
    "    \n",
    "        tstat, p_value = pvalue_diffmeans_twotail(row1['mean'], row1['std'], row1['count'], row2['mean'], row2['std'], row2['count'])\n",
    "    \n",
    "        print('Two tailed T-Test between groups: {} and {}'.format(grp1, grp2))\n",
    "        print('Diff = {} characters'.format(round(row1['mean'] - row2['mean'], 0)))\n",
    "        print('The t-stat is {} and p-value is {}'.format(round(tstat, 3), round(p_value, 3)))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some observations you might have about the above results? Are there any with large deviances that are not statistically significant at at least a 95% level? Is there any issue with using 95% as our threshold for statistical significance? In fact there is. We are running multiple hypothesis tests at once, and doing this is known to increase the probability that we have at least one false positive (i.e., $P(False Positive) = 1 - .95^{Ntests}$). We can apply a simplye but conservative method called the <a href=\"https://en.wikipedia.org/wiki/Bonferroni_correction\">Bonferoni Correction</a>, which says that if we normally would care about an alpha level of $\\alpha$ for significance testing, and we're doing $N$ tests, then our new significance level should be $\\alpha/N$. This correction is conservative because it assumes that each test is independent. Since each group is repeatedly sampled across pairs, we know that our individual tests are not indeed independent. Nonetheless, we'll see how the results hold under this new regime. \n",
    "\n",
    "Also, how do the numbers change if you rerun it using the original data, and not the cleaned data. What is the effect of outliers on the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two tailed T-Test between groups: 2-5 years, I'm getting good at what I do! and 5+ years, I'm a veteran!\n",
      "Diff = 76.0 characters\n",
      "The t-stat is 0.579 and p-value is 0.562\n",
      "\n",
      "Two tailed T-Test between groups: 2-5 years, I'm getting good at what I do! and < 2 years, I'm fresh!\n",
      "Diff = 235.0 characters\n",
      "The t-stat is 3.163 and p-value is 0.002\n",
      "\n",
      "Two tailed T-Test between groups: 2-5 years, I'm getting good at what I do! and None, I just finished my undergrad!\n",
      "Diff = 76.0 characters\n",
      "The t-stat is 0.876 and p-value is 0.381\n",
      "\n",
      "Two tailed T-Test between groups: 5+ years, I'm a veteran! and < 2 years, I'm fresh!\n",
      "Diff = 159.0 characters\n",
      "The t-stat is 1.328 and p-value is 0.184\n",
      "\n",
      "Two tailed T-Test between groups: 5+ years, I'm a veteran! and None, I just finished my undergrad!\n",
      "Diff = -0.0 characters\n",
      "The t-stat is -0.002 and p-value is 0.998\n",
      "\n",
      "Two tailed T-Test between groups: < 2 years, I'm fresh! and None, I just finished my undergrad!\n",
      "Diff = -159.0 characters\n",
      "The t-stat is -2.392 and p-value is 0.017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Rerun everything without cleaning outliers\n",
    "data_grouped = data[['len_answer', 'experience']].groupby(['experience']).agg(['mean', 'std', 'count'])\n",
    "ttest_data = data_grouped\n",
    "\n",
    "\n",
    "grps = ttest_data.index.values\n",
    "\n",
    "#Now loop through each pair\n",
    "for i, grp1 in enumerate(grps):\n",
    "    for grp2 in grps[i + 1:]:\n",
    "    \n",
    "        '''\n",
    "        hint: since the grp name is the index, pull out the record corresponding to that index value. \n",
    "        Also, the result of groupby uses a multi-index. So be sure to index on 'len_answer' as well.\n",
    "        Then pull out the mean, std, and cnt from that result.   \n",
    "        '''        \n",
    "        row1 = ttest_data.loc[grp1].loc['len_answer']\n",
    "        row2 = ttest_data.loc[grp2].loc['len_answer']\n",
    "    \n",
    "        tstat, p_value = pvalue_diffmeans_twotail(row1['mean'], row1['std'], row1['count'], row2['mean'], row2['std'], row2['count'])\n",
    "    \n",
    "        print('Two tailed T-Test between groups: {} and {}'.format(grp1, grp2))\n",
    "        print('Diff = {} characters'.format(round(row1['mean'] - row2['mean'], 0)))\n",
    "        print('The t-stat is {} and p-value is {}'.format(round(tstat, 3), round(p_value, 3)))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
