{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/briand/Desktop/ds course/ipython/data/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#We assume data is in a parallel directory to this one called 'data'\n",
    "cwd = os.getcwd()\n",
    "datadir = '/'.join(cwd.split('/')[0:-1]) + '/data/'\n",
    "print(datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Student put in read data command here:\n",
    "data = pd.read_csv(datadir + 'survey_responses_2017.csv', header = 0, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the column headers and use something more descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'cs_python', 'cs_java', 'cs_c', 'cs_perl', 'cs_javascript',\n",
       "       'cs_r', 'cs_sas', 'profile_1', 'profile_2', 'profile_3', 'profile_4',\n",
       "       'profile_5', 'profile_6', 'profile_7', 'fruit', 'len_answer', 'season',\n",
       "       'experience_coded', 'experience'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Student put in code to look at column names\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column names like 'profile_1-profile_7' aren't very descriptive. As a quick data maintenance task, let's rename the columns starting with 'profile'. The dictionary in the next cell maps the integer index to a descriptive text.\n",
    "\n",
    "Tactically, let's loop through each column name. Within the loop let's check whether the column name starts with 'profile.' If it does, let's create a new name that swaps the key with the value using profile_mapping dictionary (i.e., profile_1 -> profile_Viz). We then add the new column name to a list. If it doesn't start with 'profile' just add the old column name to the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "profile_mapping = {1:'Viz',\n",
    "                   2:'CS',\n",
    "                   3:'Math',\n",
    "                   4:'Stats',\n",
    "                   5:'ML',\n",
    "                   6:'Bus',\n",
    "                   7:'Com'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Student put code here to change the header names\n",
    "newcols = []\n",
    "\n",
    "for colname in data.columns:\n",
    "    \n",
    "    if colname[0:7] == 'profile':\n",
    "        \n",
    "        newcols.append('profile_{}'.format(profile_mapping[int(colname[-1])]))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        newcols.append(colname)\n",
    "    \n",
    "#Now swap the old columns with the values in newcols    \n",
    "data.columns = newcols    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this data to illustrate common data analytic techniques. We have one numeric variable (len_answer) and different categorical variables which may carry some signal of the 'len_answer' variable. \n",
    "\n",
    "'Len_answer' is the character count of the response to the following question: \"Besides the examples given in lecture 1, discuss a case where data science has created value for some company. Please explain the company's goals and how any sort of data analysis could have helped the company achieve said goals.\" As this is a subjective business question, let's hypothesize that students with more professional experience might be more likely to give longer answers. \n",
    "\n",
    "In more technical terms, we'll test whether the variance of len_answer can be explained away by the categorical representation of a student's experience. \n",
    "\n",
    "The first thing we should do is look at the distribution of len_answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 33.,  37.,  25.,  12.,   8.,   7.,   3.,   2.,   1.,   3.]),\n",
       " array([    0. ,   221.7,   443.4,   665.1,   886.8,  1108.5,  1330.2,\n",
       "         1551.9,  1773.6,  1995.3,  2217. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1092c0ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Student - build and plot a histogram here\n",
    "plt.hist(data.len_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we may have at least one strong outlier and a thick distribution around 0. Let's also use the Pandas describe() method to get a stronger sense of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     131.000000\n",
       "mean      535.068702\n",
       "std       458.391841\n",
       "min         0.000000\n",
       "25%       225.500000\n",
       "50%       419.000000\n",
       "75%       731.000000\n",
       "max      2217.000000\n",
       "Name: len_answer, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.len_answer.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider cleaning up the data. We'll remove the max value as well as those with a length less than 20 (which we think is a generous minimum to communicate a reasonable answer.\n",
    "\n",
    "Create a new data_frame that removes these outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118, 20)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Student create a filtered data frame here\n",
    "outlier_filter = (data.len_answer > 20) & (data.len_answer < data.len_answer.max())\n",
    "data_clean = data[outlier_filter]\n",
    "data_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have cleaned our data, let's run a pairwise t-test on each experience level to see if their difference in len_answer is statistically significant. To run a t-test, we'll need the mean, standard-deviation and count for each group. We can achieve this with a pandas groupby operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">len_answer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experience</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-5 years, I'm getting good at what I do!</th>\n",
       "      <td>826.428571</td>\n",
       "      <td>450.256594</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5+ years, I'm a veteran!</th>\n",
       "      <td>984.416667</td>\n",
       "      <td>634.752708</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt; 2 years, I'm fresh!</th>\n",
       "      <td>479.909091</td>\n",
       "      <td>340.788791</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>None, I just finished my undergrad!</th>\n",
       "      <td>446.619048</td>\n",
       "      <td>294.445241</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           len_answer                  \n",
       "                                                 mean         std count\n",
       "experience                                                             \n",
       "2-5 years, I'm getting good at what I do!  826.428571  450.256594    21\n",
       "5+ years, I'm a veteran!                   984.416667  634.752708    12\n",
       "< 2 years, I'm fresh!                      479.909091  340.788791    22\n",
       "None, I just finished my undergrad!        446.619048  294.445241    63"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Student input code here\n",
    "data_clean_grouped = data_clean[['len_answer', 'experience']].groupby(['experience']).agg(['mean', 'std', 'count'])\n",
    "data_clean_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, we can see a potential split between the [0, 2] year experience range and the [2+] experience range. Let's be more rigorous and run t-tests. Let's write a function that takes in the necessary statistics and returns a p-value.\n",
    "\n",
    "Remember, the t-stat for the difference between two means is:\n",
    "\n",
    "<center>$t = \\frac{\\hat{\\mu_1} - \\hat{\\mu_2}}{\\sqrt{\\frac{\\hat{\\sigma_1}^2}{n_1} + \\frac{\\hat{\\sigma_2}^2}{n_2}}}$</center>\n",
    "\n",
    "The p-value can be found using a t-distribution, but for simplicity, let's approximate this with the normal distribution. For the 2-tailed test, the p-value is: 2 * (1 - Norm.CDF(T))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Student complete the function\n",
    "from scipy.stats import norm\n",
    "\n",
    "def pvalue_diffmeans_twotail(mu1, sig1, n1, mu2, sig2, n2):\n",
    "    '''\n",
    "    P-value calculator for the hypothesis test of mu1 != mu2.\n",
    "    Takes in the approprate inputs to compute the t-statistic for the difference between means\n",
    "    Outputs a p-value for a two-sample t-test.\n",
    "    '''\n",
    "    diff = mu1 - mu2\n",
    "    stderror = np.sqrt(sig1**2 / n1 + sig2**2 / n2)\n",
    "    t = diff / stderror\n",
    "    \n",
    "    p_value = 2 * (1- norm.cdf(np.abs(t)))\n",
    "    \n",
    "    return (t, p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now loop through all possible pairs in data_clean_grouped and perform a t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two tailed T-Test between groups: 2-5 years, I'm getting good at what I do! and 5+ years, I'm a veteran!\n",
      "Diff = -158.0 characters\n",
      "The t-stat is -0.76 and p-value is 0.447\n",
      "\n",
      "Two tailed T-Test between groups: 2-5 years, I'm getting good at what I do! and < 2 years, I'm fresh!\n",
      "Diff = 347.0 characters\n",
      "The t-stat is 2.836 and p-value is 0.005\n",
      "\n",
      "Two tailed T-Test between groups: 2-5 years, I'm getting good at what I do! and None, I just finished my undergrad!\n",
      "Diff = 380.0 characters\n",
      "The t-stat is 3.616 and p-value is 0.0\n",
      "\n",
      "Two tailed T-Test between groups: 5+ years, I'm a veteran! and < 2 years, I'm fresh!\n",
      "Diff = 505.0 characters\n",
      "The t-stat is 2.559 and p-value is 0.01\n",
      "\n",
      "Two tailed T-Test between groups: 5+ years, I'm a veteran! and None, I just finished my undergrad!\n",
      "Diff = 538.0 characters\n",
      "The t-stat is 2.877 and p-value is 0.004\n",
      "\n",
      "Two tailed T-Test between groups: < 2 years, I'm fresh! and None, I just finished my undergrad!\n",
      "Diff = 33.0 characters\n",
      "The t-stat is 0.408 and p-value is 0.683\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Student put in code here:\n",
    "\n",
    "#get distinct values in the data frame for the experience variable\n",
    "\n",
    "#data_grouped = data[['len_answer', 'experience']].groupby(['experience']).agg(['mean', 'std', 'count'])\n",
    "#ttest_data = data_grouped\n",
    "\n",
    "\n",
    "ttest_data = data_clean_grouped\n",
    "\n",
    "\n",
    "grps = ttest_data.index.values\n",
    "\n",
    "#Now loop through each pair\n",
    "for i, grp1 in enumerate(grps):\n",
    "    for grp2 in grps[i + 1:]:\n",
    "    \n",
    "        '''\n",
    "        hint: since the grp name is the index, pull out the record corresponding to that index value. \n",
    "        Also, the result of groupby uses a multi-index. So be sure to index on 'len_answer' as well.\n",
    "        Then pull out the mean, std, and cnt from that result.   \n",
    "        '''        \n",
    "        row1 = ttest_data.ix[grp1].ix['len_answer']\n",
    "        row2 = ttest_data.ix[grp2].ix['len_answer']\n",
    "    \n",
    "        tstat, p_value = pvalue_diffmeans_twotail(row1['mean'], row1['std'], row1['count'], row2['mean'], row2['std'], row2['count'])\n",
    "    \n",
    "        print('Two tailed T-Test between groups: {} and {}'.format(grp1, grp2))\n",
    "        print('Diff = {} characters'.format(round(row1['mean'] - row2['mean'], 0)))\n",
    "        print('The t-stat is {} and p-value is {}'.format(round(tstat, 3), round(p_value, 3)))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some observations you might have about the above results? Are there any with large deviances that are not statistically significant at at least a 95% level? Is there any issue with using 95% as our threshold for statistical significance? In fact there is. We are running multiple hypothesis tests at once, and doing this is known to increase the probability that we have at least one false positive (i.e., $P(False Positive) = 1 - .95^{Ntests}$). We can apply a simplye but conservative method called the <a href=\"https://en.wikipedia.org/wiki/Bonferroni_correction\">Bonferoni Correction</a>, which says that if we normally would care about an alpha level of $\\alpha$ for significance testing, and we're doing $N$ tests, then our new significance level should be $\\alpha/N$. This correction is conservative because it assumes that each test is independent. Since each group is repeatedly sampled across pairs, we know that our individual tests are not indeed independent. Nonetheless, we'll see how the results hold under this new regime. \n",
    "\n",
    "Also, how do the numbers change if you rerun it using the original data, and not the cleaned data. What is the effect of outliers on the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two tailed T-Test between groups: 2-5 years, I'm getting good at what I do! and 5+ years, I'm a veteran!\n",
      "Diff = -230.0 characters\n",
      "The t-stat is -1.095 and p-value is 0.273\n",
      "\n",
      "Two tailed T-Test between groups: 2-5 years, I'm getting good at what I do! and < 2 years, I'm fresh!\n",
      "Diff = 199.0 characters\n",
      "The t-stat is 1.374 and p-value is 0.169\n",
      "\n",
      "Two tailed T-Test between groups: 2-5 years, I'm getting good at what I do! and None, I just finished my undergrad!\n",
      "Diff = 369.0 characters\n",
      "The t-stat is 3.393 and p-value is 0.001\n",
      "\n",
      "Two tailed T-Test between groups: 5+ years, I'm a veteran! and < 2 years, I'm fresh!\n",
      "Diff = 429.0 characters\n",
      "The t-stat is 2.043 and p-value is 0.041\n",
      "\n",
      "Two tailed T-Test between groups: 5+ years, I'm a veteran! and None, I just finished my undergrad!\n",
      "Diff = 599.0 characters\n",
      "The t-stat is 3.204 and p-value is 0.001\n",
      "\n",
      "Two tailed T-Test between groups: < 2 years, I'm fresh! and None, I just finished my undergrad!\n",
      "Diff = 170.0 characters\n",
      "The t-stat is 1.558 and p-value is 0.119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Rerun everything without cleaning outliers\n",
    "data_grouped = data[['len_answer', 'experience']].groupby(['experience']).agg(['mean', 'std', 'count'])\n",
    "ttest_data = data_grouped\n",
    "\n",
    "\n",
    "grps = ttest_data.index.values\n",
    "\n",
    "#Now loop through each pair\n",
    "for i, grp1 in enumerate(grps):\n",
    "    for grp2 in grps[i + 1:]:\n",
    "    \n",
    "        '''\n",
    "        hint: since the grp name is the index, pull out the record corresponding to that index value. \n",
    "        Also, the result of groupby uses a multi-index. So be sure to index on 'len_answer' as well.\n",
    "        Then pull out the mean, std, and cnt from that result.   \n",
    "        '''        \n",
    "        row1 = ttest_data.ix[grp1].ix['len_answer']\n",
    "        row2 = ttest_data.ix[grp2].ix['len_answer']\n",
    "    \n",
    "        tstat, p_value = pvalue_diffmeans_twotail(row1['mean'], row1['std'], row1['count'], row2['mean'], row2['std'], row2['count'])\n",
    "    \n",
    "        print('Two tailed T-Test between groups: {} and {}'.format(grp1, grp2))\n",
    "        print('Diff = {} characters'.format(round(row1['mean'] - row2['mean'], 0)))\n",
    "        print('The t-stat is {} and p-value is {}'.format(round(tstat, 3), round(p_value, 3)))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
